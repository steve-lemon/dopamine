{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final exam Bubble Bobble\n",
    "\n",
    "- Team: TToBoT\n",
    "- Member: { Sejun, Steve, Victor } @kaist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and configure levels to eval on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Import general libraries\n",
    "import gin.tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import gin\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.contrib import slim as contrib_slim\n",
    "import retro\n",
    "import gym\n",
    "import cv2\n",
    "\n",
    "# Import dopamine libraries\n",
    "import bubble\n",
    "from dopamine.colab import utils as colab_utils\n",
    "from dopamine.agents.dqn import dqn_agent\n",
    "from dopamine.agents.implicit_quantile import implicit_quantile_agent\n",
    "from dopamine.agents.rainbow import rainbow_agent\n",
    "from dopamine.discrete_domains import atari_lib\n",
    "from dopamine.discrete_domains import iteration_statistics\n",
    "from dopamine.discrete_domains import run_experiment\n",
    "from dopamine.discrete_domains.run_experiment import create_agent\n",
    "from dopamine.utils import agent_visualizer\n",
    "from dopamine.utils import atari_plotter\n",
    "from dopamine.utils import bar_plotter\n",
    "from dopamine.utils import line_plotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Runner Class\n",
    "\n",
    "The ExamRunner class allows us to run our experiment, consisting in getting the real env score for each input level number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExamRunner(run_experiment.Runner):\n",
    "    def __init__(self, base_dir, trained_agent_ckpt_path, create_agent_fn, game_levels_eval, name='', queue=None):\n",
    "        tf.logging.info('Creating ExamRunner({}) ...'.format(name))\n",
    "        self._trained_agent_ckpt_path = trained_agent_ckpt_path\n",
    "        self._use_legacy_checkpoint = False\n",
    "        super(ExamRunner, self).__init__(base_dir, create_agent_fn)\n",
    "        self._agent.eval_mode = True\n",
    "        self.game_levels_eval = game_levels_eval\n",
    "        # set highest number for self._max_steps_per_episode such that the env is free to continue until game over\n",
    "        self._max_steps_per_episode = 10000000\n",
    "        self.name = name\n",
    "        self.queue = queue\n",
    "\n",
    "    def post_reward(self, level=0, reward=0):\n",
    "        self.queue.put({'name':self.name,'level':level,'reward':reward}) if self.queue is not None else None\n",
    "    \n",
    "    def _initialize_checkpointer_and_maybe_resume(self, checkpoint_file_prefix):\n",
    "        tf.logging.info('Initializing checkpointer and resume ExamRunner ...')\n",
    "        self._agent.reload_checkpoint(self._trained_agent_ckpt_path,\n",
    "                                      self._use_legacy_checkpoint)\n",
    "        self._start_iteration = 0\n",
    "\n",
    "    def _run_one_iteration(self, game_level):\n",
    "        \"\"\"Runs one iteration of agent/environment interaction, conssisting in one episode of the agent in the environment.\n",
    "        An iteration involves running a single episode on the environment.\n",
    "        \n",
    "        Return:\n",
    "        env_true_return: int, the episode return of the original environment (not the wrapper).\n",
    "        \"\"\"\n",
    "        tf.logging.info('Starting iteration for game_level %d', game_level)\n",
    "\n",
    "        # initialize for the run\n",
    "        self._agent.eval_mode = True    \n",
    "        self._environment.episode_true_return = 0\n",
    "\n",
    "        # run one episode\n",
    "        episode_length, episode_return = self._run_one_episode()\n",
    "\n",
    "        # read from the env the true reward\n",
    "        env_true_return = self._environment.episode_true_return\n",
    "        self._environment.episode_true_return = 0\n",
    "        \n",
    "        return env_true_return\n",
    "\n",
    "    def run_experiment(self):\n",
    "        \"\"\" Run a full experiment.\n",
    "        One iteration for each evaluation game level.\n",
    "        \"\"\"\n",
    "        print(self.game_levels_eval)\n",
    "        episode_returns = list()\n",
    "        self.post_reward(0)  # post starting\n",
    "        for game_level in self.game_levels_eval:\n",
    "            print(\"========================================\")\n",
    "            tf.logging.info('Start iteration for game_level %d', game_level)\n",
    "            # choose the given game_level\n",
    "            self._environment.reset(game_level=game_level)\n",
    "\n",
    "            # run one iteration in the environment\n",
    "            env_return = self._run_one_iteration(game_level)\n",
    "            self.post_reward(game_level, env_return) # post reward\n",
    "\n",
    "            # log and save the return per episode\n",
    "            tf.logging.info('End iteration for game_level %d. Episode return: %d', game_level, env_return)\n",
    "            episode_returns.append(env_return)\n",
    "        \n",
    "        print(\"========================================\")\n",
    "        return episode_returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom IQN Agent class\n",
    "\n",
    "This new IQN Agent class only modifies the reload_checkpoint function. Anything else is the same as in the ImplicitQuantileAgent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExamIQNAgent(implicit_quantile_agent.ImplicitQuantileAgent):\n",
    "    def reload_checkpoint(self, checkpoint_path, use_legacy_checkpoint=False):\n",
    "        if use_legacy_checkpoint:\n",
    "          variables_to_restore = atari_lib.maybe_transform_variable_names(\n",
    "              tf.all_variables(), legacy_checkpoint_load=True)\n",
    "        else:\n",
    "          global_vars = set([x.name for x in tf.global_variables()])\n",
    "          ckpt_vars = [\n",
    "              '{}:0'.format(name)\n",
    "              for name, _ in tf.train.list_variables(checkpoint_path)\n",
    "          ]\n",
    "          include_vars = list(global_vars.intersection(set(ckpt_vars)))\n",
    "          variables_to_restore = contrib_slim.get_variables_to_restore(\n",
    "              include=include_vars)\n",
    "        if variables_to_restore:\n",
    "          reloader = tf.train.Saver(var_list=variables_to_restore)\n",
    "          reloader.restore(self._sess, checkpoint_path)\n",
    "          tf.logging.info('Done restoring from %s', checkpoint_path)\n",
    "        else:\n",
    "          tf.logging.info('Nothing to restore!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Gin configuration\n",
    "\n",
    "The dopamine framework works with the support of .gin files as configuration files.\n",
    "\n",
    "This configuration is taken from IQN9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin_config = '''\n",
    "# run train for bubble agent\n",
    "# - NOTE: customized for bubble w/ IQN\n",
    "# - origin from `dopamine/agents/implicit_quantile/configs/implicit_quantile.gin`\n",
    "#\n",
    "# [RUN TRAIN]\n",
    "# $ python -um dopamine.discrete_domains.train --base_dir=/tmp/bubble_iqn1 --gin_files='bubble/iqn_bubble.gin' --gin_bindings='RainbowAgent.tf_device=\"/cpu:*\"'\n",
    "\n",
    "# Hyperparameters follow Dabney et al. (2018), but we modify as necessary to\n",
    "# match those used in Rainbow (Hessel et al., 2018), to ensure apples-to-apples\n",
    "# comparison.\n",
    "import dopamine.agents.implicit_quantile.implicit_quantile_agent\n",
    "import dopamine.agents.rainbow.rainbow_agent\n",
    "import dopamine.discrete_domains.atari_lib\n",
    "import dopamine.discrete_domains.run_experiment\n",
    "import dopamine.replay_memory.prioritized_replay_buffer\n",
    "import gin.tf.external_configurables\n",
    "\n",
    "# agent for bubble\n",
    "import bubble.retro_lib_exam\n",
    "import bubble.bubble_agent\n",
    "retro_lib_exam.create_retro_environment_exam.game_name = 'BubbleBobble'\n",
    "retro_lib_exam.create_retro_environment_exam.level = 1\n",
    "Runner.create_environment_fn = @retro_lib_exam.create_retro_environment_exam\n",
    "\n",
    "\n",
    "create_agent.agent_name = 'implicit_quantile'\n",
    "RetroPreprocessingExam.wall_offset = 0          # use 200 if activate\n",
    "#RetroPreprocessingExam.step_penalty = -0.0001  # every step penalty\n",
    "RetroPreprocessingExam.step_penalty = 0.0005    # every step penalty (survival is better since 200623/443)\n",
    "RetroPreprocessingExam.reset_fire = 0\n",
    "RetroPreprocessingExam.score_bonus = 0.02       # bonus reward if got new-score.\n",
    "\n",
    "ImplicitQuantileAgent.kappa = 1.0\n",
    "ImplicitQuantileAgent.num_tau_samples = 64\n",
    "ImplicitQuantileAgent.num_tau_prime_samples = 64\n",
    "ImplicitQuantileAgent.num_quantile_samples = 32\n",
    "# ImplicitQuantileAgent.double_dqn = True   # NOTE - default is False\n",
    "RainbowAgent.gamma = 0.99\n",
    "RainbowAgent.update_horizon = 3\n",
    "RainbowAgent.min_replay_history = 20000 # agent steps\n",
    "RainbowAgent.update_period = 4\n",
    "RainbowAgent.target_update_period = 8000 # agent steps\n",
    "\n",
    "RainbowAgent.epsilon_train = 0.001\n",
    "RainbowAgent.epsilon_eval = 0.001\n",
    "RainbowAgent.epsilon_decay_period = 200000  # agent steps (1 at step=1 => 0.001 at step=200000)\n",
    "\n",
    "# IQN currently does not support prioritized replay.\n",
    "RainbowAgent.replay_scheme = 'uniform'\n",
    "RainbowAgent.tf_device = '/gpu:0'  # '/cpu:*' use for non-GPU version\n",
    "RainbowAgent.optimizer = @tf.train.AdamOptimizer()\n",
    "\n",
    "tf.train.AdamOptimizer.learning_rate = 0.00005\n",
    "tf.train.AdamOptimizer.epsilon = 0.0003125\n",
    "\n",
    "Runner.num_iterations = 600\n",
    "Runner.training_steps = 200000             # origin 250000\n",
    "Runner.evaluation_steps = 0\n",
    "Runner.max_steps_per_episode = 20000       # origin 27000\n",
    "\n",
    "WrappedPrioritizedReplayBuffer.replay_capacity = 1000000\n",
    "WrappedPrioritizedReplayBuffer.batch_size = 32\n",
    "'''\n",
    "\n",
    "# parse this config\n",
    "gin.parse_config(gin_config, skip_unknown=False)\n",
    "\n",
    "# extended gin config\n",
    "gin_files = []\n",
    "gin_bindings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment code\n",
    "\n",
    "In this section we run the experiment and get the scores for each game level\n",
    "\n",
    "- the agent is the standard dopamine IQN agent created with the create_agent() function in run_experiment.py\n",
    "- the runner is our custom ExamRunner class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_runner_fn(base_dir, trained_agent_ckpt_path, create_agent, game_levels_eval, name='', queue=None):\n",
    "    \"\"\"Creates an experiment Runner.\n",
    "    Args:\n",
    "    base_dir: str, base directory for hosting all subdirectories.\n",
    "    trained_agent_ckpt_path: load the checkpoint.\n",
    "    create_agent: function to create the agent.\n",
    "\n",
    "    Returns:\n",
    "    runner: A `Runner` like object.\n",
    "    \"\"\"\n",
    "    return ExamRunner(base_dir, trained_agent_ckpt_path, create_agent, game_levels_eval, name, queue)\n",
    "\n",
    "def create_agent_fn(sess, environment, summary_writer=None):\n",
    "    return ExamIQNAgent(sess, num_actions=environment.action_space.n, summary_writer=summary_writer)\n",
    "\n",
    "#! eval with IQN7/IQN8/IQN9 Result\n",
    "def startMyRunnerIQN(name = '', Q = None, chkpt = 100):\n",
    "    global gin_files, gin_bindings, proc_queue, GAME_LEVELS_EVAL\n",
    "    from dopamine.discrete_domains import run_experiment\n",
    "    run_experiment.load_gin_configs(gin_files, gin_bindings)\n",
    "    # create runner\n",
    "    exam = create_runner_fn(base_dir='./exam', \n",
    "                     trained_agent_ckpt_path='/tmp/bubble_{}/checkpoints/tf_ckpt-{}'.format(name, chkpt), \n",
    "                     create_agent=create_agent_fn, \n",
    "                     game_levels_eval=GAME_LEVELS_EVAL,\n",
    "                     name = name, queue = Q)\n",
    "    episode_returns = exam.run_experiment()\n",
    "    #! print the results.\n",
    "    for game_level, ep_ret in zip(GAME_LEVELS_EVAL, episode_returns):\n",
    "        print(\"Game[{}] level: {}\\tEpisode undiscounted env return: {}\".format(name, game_level, ep_ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Multi-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The IOloop is shared\n",
    "def get_ioloop():\n",
    "    import IPython, zmq\n",
    "    ipython = IPython.get_ipython()\n",
    "    if ipython and hasattr(ipython, 'kernel'):\n",
    "        return zmq.eventloop.ioloop.IOLoop.instance()\n",
    "ioloop = get_ioloop()\n",
    "\n",
    "# Thread for update canvas\n",
    "import threading, time\n",
    "from ipycanvas import Canvas\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "# Main Thread\n",
    "class MyThread(threading.Thread):\n",
    "    def __init__(self, sleep = 0.5, name = 'my'):\n",
    "        super().__init__()\n",
    "        self._quit = threading.Event()\n",
    "        self.sleep = 0.5\n",
    "        self.name = name\n",
    "        self.start()    \n",
    "    def run(self):\n",
    "        while not self._quit.isSet():\n",
    "            def update_progress():\n",
    "                if self._quit.isSet():\n",
    "                    return\n",
    "                self.display()\n",
    "            time.sleep(self.sleep)\n",
    "            ioloop.add_callback(update_progress)\n",
    "        print(\"! T[{}].Quit()\".format(self.name))\n",
    "    def quit(self):\n",
    "        self._quit.set()\n",
    "    def display(self):\n",
    "        pass\n",
    "\n",
    "# draw to canvas\n",
    "def drawPlot2Canvas(fig = None, x=0, y=0, canv = None):\n",
    "    global canvas\n",
    "    canv = canvas if canv is None else canv\n",
    "    fig = plt.gcf() if fig is None else fig\n",
    "    plt.close()          # not to update on screen.\n",
    "    fig.canvas.draw()    # draw fig to canvas\n",
    "    arr = np.array(fig.canvas.renderer._renderer)\n",
    "    h, w, d = np.shape(arr)\n",
    "    cv = Canvas(width=w, height=h)\n",
    "    cv.put_image_data(arr, 0, 0)\n",
    "    cv.stroke_rect(x,y, x+w-1, y+h-1)\n",
    "    canv.clear_rect(x,y, x+w, y+h)\n",
    "    canv.draw_image(cv, x, y)\n",
    "    \n",
    "# train thread\n",
    "def startProcessEval(target = None, name = 'T0', chkpt = 0):\n",
    "    global proc_queue, proc_list\n",
    "    proc_queue = Queue() if proc_queue is None else proc_queue\n",
    "    proc = Process(target = target, args = (name, proc_queue, chkpt))\n",
    "    proc_list.append(proc)\n",
    "    proc.start()\n",
    "    return proc\n",
    "\n",
    "# stop(or kill) processes\n",
    "def stopProcessEval():\n",
    "    global proc_list\n",
    "    for proc in proc_list:\n",
    "        t = proc.terminate()\n",
    "        proc.join()\n",
    "        print('! terminated = {}'.format(t))\n",
    "    proc_list = []\n",
    "\n",
    "# MyThread for status display\n",
    "class MyTrainStatus(MyThread):\n",
    "    draw_to_plot = True\n",
    "    def __init__(self, canvas = None):\n",
    "        super().__init__(name='status')\n",
    "        print('! MyTrainStatus({})'.format(self.name))\n",
    "        self.rewards = {}\n",
    "        self.canvas = canvas\n",
    "    def display(self):\n",
    "        global proc_queue, plt, GAME_LEVELS_EVAL\n",
    "        received = 0\n",
    "        # pop all queue...\n",
    "        while not proc_queue.empty():\n",
    "            msg = proc_queue.get()\n",
    "            n = msg['name'] if 'name' in msg else 'none'\n",
    "            l = msg['level'] if 'level' in msg else 0\n",
    "            r = msg['reward'] if 'reward' in msg else 0\n",
    "            # only if level is valid.\n",
    "            if l > 0:\n",
    "                L = self.rewards[n] if n in self.rewards else []\n",
    "                L.append(r)\n",
    "                self.rewards[n] = L\n",
    "                received += 1\n",
    "        # plot rewards if received.\n",
    "        if received > 0:\n",
    "            fig = plt.figure(1, figsize=(8, 6))\n",
    "            max_len = 0\n",
    "            for n in self.rewards:\n",
    "                l = self.rewards[n]\n",
    "                plt.plot(l, label=n)\n",
    "                max_len = max(len(l), max_len)\n",
    "            plt.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1)\n",
    "            plt.xticks([])  # hide x-ticks\n",
    "            # build tables.\n",
    "            labels = []\n",
    "            tables = []\n",
    "            cols = [GAME_LEVELS_EVAL[i] if i < len(GAME_LEVELS_EVAL) else i for i in range(max_len)]\n",
    "            for n in self.rewards:\n",
    "                labels.append(n)\n",
    "                l = self.rewards[n]\n",
    "                vals = ['%2d'%(l[i]) if i < len(l) else '' for i in range(max_len)]\n",
    "                tables.append(vals)\n",
    "            # draw table\n",
    "            plt.table(cellText=tables, rowLabels=labels, colLabels=cols, loc='bottom')\n",
    "            # print('! tables = {}'.format(tables))\n",
    "            # print('! rows = {}'.format(labels))\n",
    "            # print('! cols = {}'.format([i for i in range(max_len)]))\n",
    "            # plt.close()\n",
    "            # draw to canvas\n",
    "            drawPlot2Canvas(fig, canv = self.canvas)\n",
    "            \n",
    "# process list in global\n",
    "proc_list = []\n",
    "proc_queue = Queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Levels w/ multi-thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Game levels numbers to perform the evaluation on\n",
    "#GAME_LEVELS_EVAL = [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 20, 31, 41, 50, 60, 71, 81, 90]\n",
    "GAME_LEVELS_EVAL = [2,9,15,33,46,59,64,73,76,81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Process(Process-9, started)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating ExamRunner(iqn8) ...\n",
      "! create_retro_environment in retro_lib_exam.py: BubbleBobble/1\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level01\n",
      "INFO:tensorflow:Creating ExamRunner(iqn9) ...\n",
      "! create_retro_environment in retro_lib_exam.py: BubbleBobble/1\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level01\n",
      "! RetroPreprocessingExam: wall_offset=0, step_penalty=0.0005, game_level=1, reset_fire=0, score_bonus=0.02\n",
      "! RetroPreprocessingExam: wall_offset=0, step_penalty=0.0005, game_level=1, reset_fire=0, score_bonus=0.02\n",
      "INFO:tensorflow:Creating ExamIQNAgent agent with the following parameters:\n",
      "INFO:tensorflow:\t gamma: 0.990000\n",
      "INFO:tensorflow:\t update_horizon: 3.000000\n",
      "INFO:tensorflow:\t min_replay_history: 20000\n",
      "INFO:tensorflow:\t update_period: 4\n",
      "INFO:tensorflow:\t target_update_period: 8000\n",
      "INFO:tensorflow:\t epsilon_train: 0.001000\n",
      "INFO:tensorflow:\t epsilon_eval: 0.001000\n",
      "INFO:tensorflow:\t epsilon_decay_period: 200000\n",
      "INFO:tensorflow:\t tf_device: /gpu:0\n",
      "INFO:tensorflow:\t use_staging: True\n",
      "INFO:tensorflow:\t optimizer: <tensorflow.python.training.adam.AdamOptimizer object at 0x7f91a4122b00>\n",
      "INFO:tensorflow:\t max_tf_checkpoints_to_keep: 4\n",
      "INFO:tensorflow:Creating ExamIQNAgent agent with the following parameters:\n",
      "INFO:tensorflow:\t gamma: 0.990000\n",
      "INFO:tensorflow:Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:\n",
      "INFO:tensorflow:\t update_horizon: 3.000000\n",
      "INFO:tensorflow:\t observation_shape: (84, 84)\n",
      "INFO:tensorflow:\t min_replay_history: 20000\n",
      "INFO:tensorflow:\t observation_dtype: <class 'numpy.uint8'>\n",
      "INFO:tensorflow:\t update_period: 4\n",
      "INFO:tensorflow:\t target_update_period: 8000\n",
      "INFO:tensorflow:\t terminal_dtype: <class 'numpy.uint8'>\n",
      "INFO:tensorflow:\t epsilon_train: 0.001000\n",
      "INFO:tensorflow:\t stack_size: 4\n",
      "INFO:tensorflow:\t epsilon_eval: 0.001000\n",
      "INFO:tensorflow:\t replay_capacity: 1000000\n",
      "INFO:tensorflow:\t epsilon_decay_period: 200000\n",
      "INFO:tensorflow:\t batch_size: 32\n",
      "INFO:tensorflow:\t tf_device: /gpu:0\n",
      "INFO:tensorflow:\t update_horizon: 3\n",
      "INFO:tensorflow:\t gamma: 0.990000\n",
      "INFO:tensorflow:\t use_staging: True\n",
      "INFO:tensorflow:\t optimizer: <tensorflow.python.training.adam.AdamOptimizer object at 0x7f91a4122b00>\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/dopamine_rl-3.0.1-py3.6.egg/dopamine/replay_memory/circular_replay_buffer.py:821: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    INFO:tensorflow:\t max_tf_checkpoints_to_keep: 4\n",
      "\n",
      "INFO:tensorflow:Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:\n",
      "INFO:tensorflow:\t observation_shape: (84, 84)\n",
      "INFO:tensorflow:\t observation_dtype: <class 'numpy.uint8'>\n",
      "INFO:tensorflow:\t terminal_dtype: <class 'numpy.uint8'>\n",
      "INFO:tensorflow:\t stack_size: 4\n",
      "INFO:tensorflow:\t replay_capacity: 1000000\n",
      "INFO:tensorflow:\t batch_size: 32\n",
      "INFO:tensorflow:\t update_horizon: 3\n",
      "INFO:tensorflow:\t gamma: 0.990000\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/dopamine_rl-3.0.1-py3.6.egg/dopamine/replay_memory/circular_replay_buffer.py:821: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/dopamine_rl-3.0.1-py3.6.egg/dopamine/discrete_domains/atari_lib.py:418: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/dopamine_rl-3.0.1-py3.6.egg/dopamine/discrete_domains/atari_lib.py:418: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/dopamine_rl-3.0.1-py3.6.egg/dopamine/agents/implicit_quantile/implicit_quantile_agent.py:192: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/dopamine_rl-3.0.1-py3.6.egg/dopamine/agents/implicit_quantile/implicit_quantile_agent.py:192: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/dopamine_rl-3.0.1-py3.6.egg/dopamine/agents/dqn/dqn_agent.py:206: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "INFO:tensorflow:legacy_checkpoint_load: False\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/dopamine_rl-3.0.1-py3.6.egg/dopamine/agents/dqn/dqn_agent.py:206: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "INFO:tensorflow:legacy_checkpoint_load: False\n",
      "INFO:tensorflow:\t kappa: 1.000000\n",
      "INFO:tensorflow:\t num_tau_samples: 64\n",
      "INFO:tensorflow:\t num_tau_prime_samples: 64\n",
      "INFO:tensorflow:\t num_quantile_samples: 32\n",
      "INFO:tensorflow:\t quantile_embedding_dim: 64\n",
      "INFO:tensorflow:\t double_dqn: False\n",
      "INFO:tensorflow:\t kappa: 1.000000\n",
      "INFO:tensorflow:\t num_tau_samples: 64\n",
      "INFO:tensorflow:\t num_tau_prime_samples: 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t num_quantile_samples: 32\n",
      "INFO:tensorflow:\t quantile_embedding_dim: 64\n",
      "INFO:tensorflow:\t double_dqn: False\n",
      "INFO:tensorflow:Initializing checkpointer and resume ExamRunner ...\n",
      "INFO:tensorflow:Initializing checkpointer and resume ExamRunner ...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/bubble_iqn8/checkpoints/tf_ckpt-654\n",
      "INFO:tensorflow:Restoring parameters from /tmp/bubble_iqn9/checkpoints/tf_ckpt-304\n",
      "INFO:tensorflow:Done restoring from /tmp/bubble_iqn8/checkpoints/tf_ckpt-654\n",
      "[2, 9, 15, 33, 46, 59, 64, 73, 76, 81]\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 2\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level02\n",
      "INFO:tensorflow:Done restoring from /tmp/bubble_iqn9/checkpoints/tf_ckpt-304\n",
      "[2, 9, 15, 33, 46, 59, 64, 73, 76, 81]\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 2\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level02\n",
      "INFO:tensorflow:Starting iteration for game_level 2\n",
      "INFO:tensorflow:Starting iteration for game_level 2\n",
      "INFO:tensorflow:End iteration for game_level 2. Episode return: 5\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 9\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level09\n",
      "INFO:tensorflow:Starting iteration for game_level 9\n",
      "INFO:tensorflow:End iteration for game_level 9. Episode return: 2\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 15\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level15\n",
      "INFO:tensorflow:Starting iteration for game_level 15\n",
      "INFO:tensorflow:End iteration for game_level 15. Episode return: 0\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 33\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level33\n",
      "INFO:tensorflow:Starting iteration for game_level 33\n",
      "INFO:tensorflow:End iteration for game_level 2. Episode return: 14\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 9\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level09\n",
      "INFO:tensorflow:End iteration for game_level 33. Episode return: 0\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 46\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level46\n",
      "INFO:tensorflow:Starting iteration for game_level 9\n",
      "INFO:tensorflow:Starting iteration for game_level 46\n",
      "INFO:tensorflow:End iteration for game_level 46. Episode return: 1\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 59\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level59\n",
      "INFO:tensorflow:Starting iteration for game_level 59\n",
      "INFO:tensorflow:End iteration for game_level 59. Episode return: 2\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 64\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level64\n",
      "INFO:tensorflow:Starting iteration for game_level 64\n",
      "INFO:tensorflow:End iteration for game_level 9. Episode return: 5\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 15\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level15\n",
      "INFO:tensorflow:Starting iteration for game_level 15\n",
      "INFO:tensorflow:End iteration for game_level 64. Episode return: 6\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 73\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level73\n",
      "INFO:tensorflow:Starting iteration for game_level 73\n",
      "INFO:tensorflow:End iteration for game_level 73. Episode return: 1\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 76\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level76\n",
      "INFO:tensorflow:Starting iteration for game_level 76\n",
      "INFO:tensorflow:End iteration for game_level 76. Episode return: 3\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 81\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level81\n",
      "INFO:tensorflow:Starting iteration for game_level 81\n",
      "INFO:tensorflow:End iteration for game_level 81. Episode return: 1\n",
      "========================================\n",
      "Game[iqn9] level: 2\tEpisode undiscounted env return: 5.0\n",
      "Game[iqn9] level: 9\tEpisode undiscounted env return: 2.0\n",
      "Game[iqn9] level: 15\tEpisode undiscounted env return: 0.0\n",
      "Game[iqn9] level: 33\tEpisode undiscounted env return: 0.0\n",
      "Game[iqn9] level: 46\tEpisode undiscounted env return: 1.0\n",
      "Game[iqn9] level: 59\tEpisode undiscounted env return: 2.0\n",
      "Game[iqn9] level: 64\tEpisode undiscounted env return: 6.0\n",
      "Game[iqn9] level: 73\tEpisode undiscounted env return: 1.0\n",
      "Game[iqn9] level: 76\tEpisode undiscounted env return: 3.0\n",
      "Game[iqn9] level: 81\tEpisode undiscounted env return: 1.0\n",
      "INFO:tensorflow:End iteration for game_level 15. Episode return: 0\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 33\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level33\n",
      "INFO:tensorflow:Starting iteration for game_level 33\n",
      "INFO:tensorflow:End iteration for game_level 33. Episode return: 4\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 46\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level46\n",
      "INFO:tensorflow:Starting iteration for game_level 46\n",
      "INFO:tensorflow:End iteration for game_level 46. Episode return: 4\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 59\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level59\n",
      "INFO:tensorflow:Starting iteration for game_level 59\n",
      "INFO:tensorflow:End iteration for game_level 59. Episode return: 7\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 64\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level64\n",
      "INFO:tensorflow:Starting iteration for game_level 64\n",
      "INFO:tensorflow:End iteration for game_level 64. Episode return: 6\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 73\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level73\n",
      "INFO:tensorflow:Starting iteration for game_level 73\n",
      "INFO:tensorflow:End iteration for game_level 73. Episode return: 5\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 76\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level76\n",
      "INFO:tensorflow:Starting iteration for game_level 76\n",
      "INFO:tensorflow:End iteration for game_level 76. Episode return: 3\n",
      "========================================\n",
      "INFO:tensorflow:Start iteration for game_level 81\n",
      "INFO:tensorflow:Create RetroGame in RetroPreprocessingExam:BubbleBobble-Nes w/ stage:Level81\n",
      "INFO:tensorflow:Starting iteration for game_level 81\n",
      "INFO:tensorflow:End iteration for game_level 81. Episode return: 0\n",
      "========================================\n",
      "Game[iqn8] level: 2\tEpisode undiscounted env return: 14.0\n",
      "Game[iqn8] level: 9\tEpisode undiscounted env return: 5.0\n",
      "Game[iqn8] level: 15\tEpisode undiscounted env return: 0.0\n",
      "Game[iqn8] level: 33\tEpisode undiscounted env return: 4.0\n",
      "Game[iqn8] level: 46\tEpisode undiscounted env return: 4.0\n",
      "Game[iqn8] level: 59\tEpisode undiscounted env return: 7.0\n",
      "Game[iqn8] level: 64\tEpisode undiscounted env return: 6.0\n",
      "Game[iqn8] level: 73\tEpisode undiscounted env return: 5.0\n",
      "Game[iqn8] level: 76\tEpisode undiscounted env return: 3.0\n",
      "Game[iqn8] level: 81\tEpisode undiscounted env return: 0.0\n"
     ]
    }
   ],
   "source": [
    "#! start process of runner\n",
    "#startProcessEval(target = startMyRunnerIQN, name = 'iqn7', chkpt = 199)\n",
    "startProcessEval(target = startMyRunnerIQN, name = 'iqn8', chkpt = 654)\n",
    "startProcessEval(target = startMyRunnerIQN, name = 'iqn9', chkpt = 304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! MyTrainStatus(status)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd12b963b74413eb84f520f29527b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(height=480, width=640)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#! show plot display \n",
    "canvas = Canvas(width=640, height=480)\n",
    "tstatus = MyTrainStatus(canvas)\n",
    "canvas\n",
    "#! \n",
    "# iqn7 - checkpoint by 199 iteration of training on only stage-level=1\n",
    "# iqn8 - checkpoint by 654 iteration of training with mixed strategy (see presentation)\n",
    "# iqn9 - checkpoint by 304 iteration of training on each stage-level=1~50 (since 200 after iqn7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! terminated = None\n",
      "! terminated = None\n"
     ]
    }
   ],
   "source": [
    "# stop - thread of status\n",
    "tstatus.quit() if tstatus else None\n",
    "# stop all process of eval\n",
    "stopProcessEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
